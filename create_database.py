# create_database_local.py
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_ollama import OllamaLLM
from pdf2image import convert_from_path
import pytesseract
import os
import shutil
import sys
import warnings
import logging

# --- C·∫•u h√¨nh & H·∫±ng s·ªë ---
warnings.filterwarnings("ignore")
logging.getLogger("langchain").setLevel(logging.ERROR)

CHROMA_PATH = "chroma"
DATA_PATH = "data/quy-che-hoc-vu-ctu.pdf"

# Embedding local
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
EMBEDDING_DEVICE = "cpu"  # ho·∫∑c "cuda"

# Chia ƒëo·∫°n vƒÉn
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 100
MIN_CHUNK_LENGTH = 50
BATCH_SIZE = 200

# M√¥ h√¨nh local ƒë·ªÉ l√†m s·∫°ch OCR (qua Ollama)
LLM_MODEL = "llama3"
# --- K·∫øt th√∫c c·∫•u h√¨nh ---


def main():
    print("\n=== B·∫ÆT ƒê·∫¶U T·∫†O DATABASE CHO RAG ===\n")

    # 1Ô∏è‚É£ OCR PDF scan ‚Üí text
    documents = load_documents_from_scanned_pdf()

    # 2Ô∏è‚É£ L√†m s·∫°ch text b·∫±ng LLaMA3 local
    documents = clean_text_with_llm(documents)

    # 3Ô∏è‚É£ Chia nh·ªè vƒÉn b·∫£n
    chunks = split_text(documents)

    # 4Ô∏è‚É£ L∆∞u v√†o Chroma
    save_to_chroma(chunks)

    print("\n=== HO√ÄN T·∫§T T·∫†O DATABASE ===\n")


def load_documents_from_scanned_pdf() -> list[Document]:
    """OCR PDF scan th√†nh text."""
    print(f"‚Üí ƒêang t·∫£i file PDF: {DATA_PATH}")
    if not os.path.exists(DATA_PATH):
        print(f"[L·ªñI] Kh√¥ng t√¨m th·∫•y file t·∫°i: {DATA_PATH}")
        sys.exit(1)

    try:
        images = convert_from_path(DATA_PATH)
    except Exception as e:
        print(f"[L·ªñI] L·ªói khi chuy·ªÉn PDF sang ·∫£nh: {e}")
        print("üí° C√†i poppler-utils: sudo apt install poppler-utils")
        sys.exit(1)

    documents = []
    print(f"‚Üí ƒêang OCR {len(images)} trang PDF...")

    for i, image in enumerate(images):
        page_num = i + 1
        try:
            text = pytesseract.image_to_string(image, lang="vie").strip()
            if len(text) < 10:
                print(f"‚ö†Ô∏è  Trang {page_num}: OCR r·∫•t √≠t k√Ω t·ª±.")
            metadata = {"source": DATA_PATH, "page": page_num}
            documents.append(Document(page_content=text, metadata=metadata))
        except Exception as e:
            print(f"[L·ªñI] OCR l·ªói ·ªü trang {page_num}: {e}")

    if not documents:
        print("[L·ªñI] Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c trang n√†o.")
        sys.exit(1)

    print(f"‚úÖ OCR ho√†n t·∫•t: {len(documents)} trang ƒë√£ x·ª≠ l√Ω.\n")
    return documents


def clean_text_with_llm(documents: list[Document]) -> list[Document]:
    """
    D√πng m√¥ h√¨nh local LLaMA3 ƒë·ªÉ hi·ªáu ch·ªânh text OCR.
    """
    print("‚Üí ƒêang hi·ªáu ch·ªânh vƒÉn b·∫£n OCR b·∫±ng m√¥ h√¨nh local (LLaMA3)...")

    try:
        llm = OllamaLLM(model=LLM_MODEL)
    except Exception as e:
        print(f"[L·ªñI] Kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c Ollama ho·∫∑c model {LLM_MODEL}: {e}")
        return documents

    cleaned_docs = []
    for i, doc in enumerate(documents):
        prompt = f"""
        H√£y ch·ªânh s·ª≠a vƒÉn b·∫£n ti·∫øng Vi·ªát sau cho r√µ r√†ng, ƒë√∫ng ch√≠nh t·∫£,
        gi·ªØ nguy√™n n·ªôi dung g·ªëc (kh√¥ng th√™m th√¥ng tin m·ªõi):
        ---
        {doc.page_content}
        ---
        """
        try:
            response = llm.invoke(prompt).strip()
            cleaned_docs.append(Document(page_content=response, metadata=doc.metadata))
            print(f"‚úÖ Trang {doc.metadata['page']} ƒë√£ l√†m s·∫°ch.")
        except Exception as e:
            print(f"‚ö†Ô∏è  L·ªói khi l√†m s·∫°ch trang {doc.metadata['page']}: {e}")
            cleaned_docs.append(doc)

    print(f"‚úÖ Ho√†n t·∫•t l√†m s·∫°ch {len(cleaned_docs)} trang.\n")
    return cleaned_docs


def split_text(documents: list[Document]) -> list[Document]:
    """Chia vƒÉn b·∫£n th√†nh c√°c chunk nh·ªè."""
    print("‚Üí ƒêang chia vƒÉn b·∫£n th√†nh c√°c ƒëo·∫°n nh·ªè...")

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        length_function=len,
        add_start_index=True,
    )

    chunks = splitter.split_documents(documents)
    print(f"‚Üí T·ªïng s·ªë chunk tr∆∞·ªõc khi l·ªçc: {len(chunks)}")

    valid_chunks = [
        c for c in chunks if c.page_content.strip() and len(c.page_content) > MIN_CHUNK_LENGTH
    ]
    print(f"‚úÖ Sau khi l·ªçc: {len(valid_chunks)} chunks h·ª£p l·ªá.\n")

    if not valid_chunks:
        print("[L·ªñI] Kh√¥ng c√≥ chunk h·ª£p l·ªá.")
        sys.exit(1)

    return valid_chunks


def save_to_chroma(chunks: list[Document]):
    """T·∫°o vector database b·∫±ng Chroma."""
    print("‚Üí Chu·∫©n b·ªã l∆∞u v√†o Chroma DB...")

    if os.path.exists(CHROMA_PATH):
        print(f"üóëÔ∏è  X√≥a database c≈©: {CHROMA_PATH}")
        shutil.rmtree(CHROMA_PATH)

    print(f"‚Üí T·∫£i model embedding: {EMBEDDING_MODEL}")
    try:
        embedding_fn = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL,
            model_kwargs={"device": EMBEDDING_DEVICE},
            encode_kwargs={"normalize_embeddings": True},
        )
    except Exception as e:
        print(f"[L·ªñI] Kh√¥ng th·ªÉ t·∫£i model embedding: {e}")
        sys.exit(1)

    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_fn)
    total_chunks = len(chunks)
    total_batches = (total_chunks - 1) // BATCH_SIZE + 1

    for i in range(0, total_chunks, BATCH_SIZE):
        batch = chunks[i : i + BATCH_SIZE]
        print(f"‚Üí Th√™m l√¥ {i//BATCH_SIZE + 1}/{total_batches}...")
        try:
            db.add_documents(batch)
        except Exception as e:
            print(f"[L·ªñI] Khi th√™m batch: {e}")
            continue

    db.persist()
    print(f"‚úÖ ƒê√£ l∆∞u {len(chunks)} chunks v√†o {CHROMA_PATH}.\n")


if __name__ == "__main__":
    main()
